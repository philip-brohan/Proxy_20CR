
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Machine Learning for Data Assimilation &#8212; Machine Learning for Data Assimilation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinxdoc.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Building a generative model" href="model.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="model.html" title="Building a generative model"
             accesskey="N">next</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">ML for DA</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Machine Learning for Data Assimilation</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="machine-learning-for-data-assimilation">
<h1>Machine Learning for Data Assimilation<a class="headerlink" href="#machine-learning-for-data-assimilation" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://reanalyses.org">Reanalysis</a> is awesome, but it’s very slow and very expensive. We can make it dramatically easier and cheaper with <a class="reference external" href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a>.</p>
<div class="figure align-center" style="width: 95%">
<a class="reference internal image-reference" href="_images/Intro_figure.jpg"><img alt="_images/Intro_figure.jpg" src="_images/Intro_figure.jpg" style="width: 95%;" /></a>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>We would like to know the weather everywhere in the world, for every hour in the last 100 years at least. But for most times, and most places, we have no observations of the weather. So we need to use the observations we do have as efficiently as possible - we need to make each observation inform our estimates of the weather in places remote from the observation. A powerful technique for this is <a class="reference external" href="https://en.wikipedia.org/wiki/Data_assimilation">Data Assimilation (DA)</a> which starts from a model of the weather, and uses observations to constrain the state of the model. Using DA with <a class="reference external" href="https://en.wikipedia.org/wiki/General_circulation_model">General circulation Models (GCMs)</a> has been enormously successful, providing precise and accurate estimates of global weather, operational weather forecasts, comprehensive modern reanalyses such as <a class="reference external" href="https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5">ERA5</a> and long sparse-observation reanalyses such as the <a class="reference external" href="https://psl.noaa.gov/data/20thC_Rean/">Twentieth Century Reanalysis (20CR)</a>.
But GCMs are complex to use and expensive to run. Reanalysis projects require specialist expertise and enormous quantities of supercomputer time, so this technology, despite its power, is not very widely used. We already know that that we can use Machine Learning (ML) to make <a class="reference external" href="https://brohan.org/ML_GCM/">fast approximations to a GCM</a>, can we extend this to do DA as well?</p>
<p>Here I show that you can use a <a class="reference external" href="https://en.wikipedia.org/wiki/Variational_autoencoder">Variational AutoEncoder (VAE)</a> to build a fast <a class="reference external" href="https://en.wikipedia.org/wiki/Generative_model#Deep_generative_models">deep generative model</a> linking physically-plausible weather fields to a complete, continuous, low-dimensional latent space. Data Assimilation can then be done by searching the latent space for the state that maximises the fit between the linked field and the observations. The DA process takes about 1 minute on a standard laptop.</p>
</div>
<div class="section" id="finding-an-atmospheric-state-that-matches-observations">
<h2>Finding an atmospheric state that matches observations<a class="headerlink" href="#finding-an-atmospheric-state-that-matches-observations" title="Permalink to this headline">¶</a></h2>
<p>Suppose we have a representation of the atmospheric state <span class="math notranslate nohighlight">\(z\)</span>, and some observations <span class="math notranslate nohighlight">\(y^o\)</span>. We’d like to know what values of <span class="math notranslate nohighlight">\(z\)</span> are consistent with <span class="math notranslate nohighlight">\(y^o\)</span> - we want the probability distribution function <span class="math notranslate nohighlight">\(p(z|y^o)\)</span> (or at least the value of <span class="math notranslate nohighlight">\(z\)</span> that maximises this probability - the best-estimate atmospheric state given the observations).</p>
<p>From Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[p(z|y^o) = p(y^o|z)p(z)/p(y^o)\]</div>
<p>where <span class="math notranslate nohighlight">\(p(z)\)</span> is our prior estimate of <span class="math notranslate nohighlight">\(z\)</span> (before we got the observations) - sometimes called a “background” estimate. And <span class="math notranslate nohighlight">\(p(y^o)\)</span> is our prior probability of observations <span class="math notranslate nohighlight">\(y^o\)</span>.</p>
<p>Start by assuming <span class="math notranslate nohighlight">\(p(y^o)\)</span> is constant - all values of observations are equally likely.</p>
<p>To calculate <span class="math notranslate nohighlight">\(p(y|z)\)</span> we need an observations operator <span class="math notranslate nohighlight">\(H\)</span>: <span class="math notranslate nohighlight">\(y=H(z)\)</span>. Suppose <span class="math notranslate nohighlight">\(y^o\)</span> and the corresponding <span class="math notranslate nohighlight">\(H(z)\)</span> contain Gaussian errors with covariance <span class="math notranslate nohighlight">\(R\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[p(y^o|z) \propto \exp(-1/2(y^o-H(z))^TR^{-1}(y^o-H(z))\]</div>
<p>We also need a prior estimate <span class="math notranslate nohighlight">\(p(z)\)</span> for the atmospheric state. This is a property of the model we are using to represent the atmosphere. Our atmospheric state <span class="math notranslate nohighlight">\(z\)</span> will be the output of a model (call this <span class="math notranslate nohighlight">\(g\)</span>):</p>
<div class="math notranslate nohighlight">
\[z = g(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the state vector of the model. A useful model <span class="math notranslate nohighlight">\(g()\)</span> will have three properties:</p>
<ul class="simple">
<li><p>It will produce precise and accurate estimates of the atmospheric state <span class="math notranslate nohighlight">\(z\)</span>. In practice this means <span class="math notranslate nohighlight">\(p(y^o|z)\)</span> is small compared to alternative models.</p></li>
<li><p>It will be cheap and easy to estimate <span class="math notranslate nohighlight">\(p(y^o|z)\)</span> - how well the model fits the observations.</p></li>
<li><p>It will be cheap and easy to estimate <span class="math notranslate nohighlight">\(p(z)\)</span> - the prior estimate of the atmospheric state.</p></li>
</ul>
<p>Modern <a class="reference external" href="https://en.wikipedia.org/wiki/General_circulation_model">GCMs</a> score very highly on the first of these points - they make very precise and accurate estimates of the atmospheric state vector - but nothing about them is cheap, or easy. On the other hand, simple statistical models can make fast estimates of <span class="math notranslate nohighlight">\(p(y^o|z)\)</span> and <span class="math notranslate nohighlight">\(p(z)\)</span> (and so allow fast estimates of the <span class="math notranslate nohighlight">\(z\)</span> that maximises <span class="math notranslate nohighlight">\(p(z|y^o)\)</span>) but the quality of the atmospheric states they produce are unsatisfactory.</p>
<p>Modern <a class="reference external" href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning (ML)</a> offers us the chance to square this circle: simple <span class="math notranslate nohighlight">\(p(z)\)</span>, fast estimates of <span class="math notranslate nohighlight">\(p(y^o|z)\)</span> (and so <span class="math notranslate nohighlight">\(p(z|y^o)\)</span>), and good quality atmospheric state estimates. Speed, accuracy, and precision.</p>
</div>
<div class="section" id="a-concrete-example-2-metre-air-temperature">
<h2>A concrete example: 2-metre air temperature<a class="headerlink" href="#a-concrete-example-2-metre-air-temperature" title="Permalink to this headline">¶</a></h2>
<p>As a concrete example, we will use not the whole weather state vector, but just the 2-metre air temperature anomaly (as our <span class="math notranslate nohighlight">\(z\)</span>). And we will use station observations of 2-metre temperature to constrain it.  So there is some function <span class="math notranslate nohighlight">\(f()\)</span> where:</p>
<div class="figure align-center" style="width: 95%">
<a class="reference internal image-reference" href="_images/f_obs_map_to_field.jpg"><img alt="_images/f_obs_map_to_field.jpg" src="_images/f_obs_map_to_field.jpg" style="width: 95%;" /></a>
</div>
<p>Or</p>
<div class="figure align-center" style="width: 95%">
<a class="reference internal image-reference" href="_images/f_obs_txt_to_field.jpg"><img alt="_images/f_obs_txt_to_field.jpg" src="_images/f_obs_txt_to_field.jpg" style="width: 95%;" /></a>
</div>
<p>Where each <span class="math notranslate nohighlight">\(o\)</span> is a triplet of (lat, lon, T2m anomaly) and <span class="math notranslate nohighlight">\(n\)</span> is a variable - we need to be able to use any number of observations. The challenge is to use ML to make a suitable <span class="math notranslate nohighlight">\(f()\)</span>, that is, one where <span class="math notranslate nohighlight">\(f(y^o) = z^o\)</span>, where <span class="math notranslate nohighlight">\(z^o\)</span> is the value of <span class="math notranslate nohighlight">\(z\)</span> that maximises <span class="math notranslate nohighlight">\(p(z|y^o)\)</span></p>
<p>This example has the virtue that the observations operator <span class="math notranslate nohighlight">\(H(z)\)</span> is very simple - we just interpolate <span class="math notranslate nohighlight">\(z\)</span> to the locations of the observations. Also we can reasonably assume the observations errors are constant and uncorrelated, so maximising <span class="math notranslate nohighlight">\(p(y^o|z)\)</span> is the same as minimising the RMS difference between <span class="math notranslate nohighlight">\(H(z)\)</span> and <span class="math notranslate nohighlight">\(y^o\)</span>. That is, between the observations and the field <span class="math notranslate nohighlight">\(z\)</span> at the locations of the observations.</p>
</div>
<div class="section" id="generative-model-and-latent-space">
<h2>Generative model, and latent space<a class="headerlink" href="#generative-model-and-latent-space" title="Permalink to this headline">¶</a></h2>
<p>Traditionally, we would use a GCM to generate plausible temperature fields (<span class="math notranslate nohighlight">\(z\)</span>). Here, we are going to replace the GCM with a generative model: We need a model <span class="math notranslate nohighlight">\(z=g(x)\)</span>, where a cheap and simple prior input distribution <span class="math notranslate nohighlight">\(p(x)\)</span> produces a wide range of good quality output temperature fields (the same sort of output a good GCM might produce, or that we see in observations).</p>
<p>So let’s say that the model state vector <span class="math notranslate nohighlight">\(x\)</span> has 100-elements, and the prior <span class="math notranslate nohighlight">\(p(x)\)</span> is a multivariate normal distribution with mean 0 and variance 1.</p>
<div class="figure align-center" style="width: 95%">
<a class="reference internal image-reference" href="_images/g_ls_to_t2m.jpg"><img alt="_images/g_ls_to_t2m.jpg" src="_images/g_ls_to_t2m.jpg" style="width: 95%;" /></a>
</div>
<p>This function is called a generator, and the  inputs <span class="math notranslate nohighlight">\(x\)</span> form a vector in 100-dimensional space (the <em>latent space</em>). There are three additional requirements on <span class="math notranslate nohighlight">\(g()\)</span>: We want to be able to perturb the state, so it must be continuous - a small change in the latent space (<span class="math notranslate nohighlight">\(x\)</span>) must result in a small change in the output (<span class="math notranslate nohighlight">\(z\)</span>). We want to sample freely from the prior, so it must be complete - any point in the latent space (sampled from the prior) must map into a plausible temperature anomaly field. And it must be <em>fast</em> (otherwise, we could just use a GCM).</p>
<p>This function is specified to meet the three requirements above (good quality <span class="math notranslate nohighlight">\(z\)</span>, cheap and easy <span class="math notranslate nohighlight">\(p(z)\)</span>, and cheap and easy <span class="math notranslate nohighlight">\(p(y^o|z)\)</span>). It would be almost impossible to code a function meeting this specification, but we don’t have to write the function, we can <em>learn</em> it.</p>
</div>
<div class="section" id="learning-a-generator-with-a-variational-autoencoder">
<h2>Learning a generator with a Variational AutoEncoder<a class="headerlink" href="#learning-a-generator-with-a-variational-autoencoder" title="Permalink to this headline">¶</a></h2>
<p>We can create a generator with exactly these properties, using a <a class="reference external" href="https://en.wikipedia.org/wiki/Variational_autoencoder">Variational AutoEncoder (VAE)</a>. An autoencoder is a pair of neural nets: one of them (the encoder) compresses an input field into a low-dimensional latent space, and the other (the generator) expands the small latent space representation back into the input field. They are trained as a pair - optimising to make generator(encoder(input)) as close to the original input as possible. A variational autoencoder adds two complications to the basic autoencoder: The encoder outputs a distribution in latent space not a point (the distribution is parametrized by its mean <span class="math notranslate nohighlight">\(m\)</span> and standard deviation <span class="math notranslate nohighlight">\(s\)</span>) and it is trained not just to produce the desired output, but also to produce the desired distribution in latent space (<span class="math notranslate nohighlight">\(p(x)\)</span>) - a multi-variate unit normal distribution.</p>
<div class="figure align-center" style="width: 95%">
<a class="reference internal image-reference" href="_images/DCVAE.jpg"><img alt="_images/DCVAE.jpg" src="_images/DCVAE.jpg" style="width: 95%;" /></a>
</div>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="model.html">Details: Building and training the VAE</a></li>
</ul>
</div>
<p>The generator produced by training meets the requirements specified above:</p>
<ul class="simple">
<li><p>It makes good quality <span class="math notranslate nohighlight">\(z\)</span> as output, because we train it on examples of good quality <span class="math notranslate nohighlight">\(z\)</span> - here temperature anomaly fields from <a class="reference external" href="https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5">ERA5</a>.</p></li>
<li><p>It has a cheap and easy <span class="math notranslate nohighlight">\(p(z)\)</span>, because we train <span class="math notranslate nohighlight">\(p(x)\)</span> (and so <span class="math notranslate nohighlight">\(p(z)\)</span>) to be a multi-variate unit normal distribution.</p></li>
<li><p>It is continuous and complete because of the variational construction (noise in the encoder output, and constraint on the latent space distribution).</p></li>
<li><p>It is fast because the generator is implemented as a <a class="reference external" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural net (CNN)</a>. (It runs in much less than one second).</p></li>
<li><p>It has cheap and easy <span class="math notranslate nohighlight">\(p(y^o|z)\)</span> because it is fast and complete - it’s cheap and easy to make samples of <span class="math notranslate nohighlight">\(z\)</span> and so <span class="math notranslate nohighlight">\(H(z)\)</span>.</p></li>
</ul>
<p>So as long as as the VAE can be trained successfully on good quality data (examples of the sort of <span class="math notranslate nohighlight">\(z\)</span> that we want it to produce), the generator learned by the VAE will be exactly the function <span class="math notranslate nohighlight">\(g()\)</span> that we need. And it does work: training a simple VAE on 40 years of daily T2m anomalies (from ERA5) gives decent results after only about 10 minutes (on a single v100 GPU).</p>
<div class="figure align-center" id="id3" style="width: 95%">
<a class="reference internal image-reference" href="_images/DCVAE_validation.jpg"><img alt="_images/DCVAE_validation.jpg" src="_images/DCVAE_validation.jpg" style="width: 95%;" /></a>
<p class="caption"><span class="caption-text">VAE validation: top left - original field, top right - generator output, bottom left - difference, bottom right - scatter original::output. (Note that a substantially better result could be produced with more model-building effort and a larger latent space, but this is good enough for present purposes).</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>The VAE is serving as a generator factory - learning a generator function <span class="math notranslate nohighlight">\(g()\)</span> that has all the properties we need, from observations of the weather variable of interest (T2m anomalies from ERA5).</p>
<div class="figure align-center" style="width: 95%">
<a class="reference internal image-reference" href="_images/DCVAE_to_g_latent.jpg"><img alt="_images/DCVAE_to_g_latent.jpg" src="_images/DCVAE_to_g_latent.jpg" style="width: 95%;" /></a>
</div>
</div>
<div class="section" id="assimilating-through-the-latent-space">
<h2>Assimilating through the latent space<a class="headerlink" href="#assimilating-through-the-latent-space" title="Permalink to this headline">¶</a></h2>
<p>But we don’t want <span class="math notranslate nohighlight">\(g()\)</span> - the function making a weather field from a latent space vector; we want <span class="math notranslate nohighlight">\(f()\)</span> - a function making a weather field from a variable set of observations. The virtue of having <span class="math notranslate nohighlight">\(g()\)</span>, is that it allows us to convert the problem from finding a weather field that matches the observations, to finding a point in latent space that generates a weather field that matches the observations - we map observations to latent space vector to field. That is, we can look for the solution in the latent space, rather than in the real-space weather field.</p>
<div class="figure align-center" style="width: 95%">
<a class="reference internal image-reference" href="_images/f_obs_to_latent_to_field.jpg"><img alt="_images/f_obs_to_latent_to_field.jpg" src="_images/f_obs_to_latent_to_field.jpg" style="width: 95%;" /></a>
</div>
<p>We want to find the value of <span class="math notranslate nohighlight">\(z\)</span> which maximises:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(z|y^o) &amp; = p(y^o|z)p(z)/p(y^o) \\
         &amp; \propto \exp(-{1\over2}(y^o-H(z))^TR^{-1}(y^o-H(z))p(z) \\
         &amp; \approx \exp(-{1\over2}(y^o-H(z))^2)p(z)\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(z=g(x)\)</span> so we need the value of x which maximises:</p>
<div class="math notranslate nohighlight">
\[\exp(-{1\over2}(y^o-H(g(x)))^2)p(x)\]</div>
<p>We have <span class="math notranslate nohighlight">\(g(x)\)</span> - the generator we learned with the VAE; <span class="math notranslate nohighlight">\(p(x)\)</span> is a unit normal distribution; and <span class="math notranslate nohighlight">\(H(g(x))\)</span> is very simple - we just interpolate the generator output to the locations of the observations. So we can calculate this for any <span class="math notranslate nohighlight">\(x\)</span> and finding the maximum is just a matter of optimisation.
Because <span class="math notranslate nohighlight">\(g(x)\)</span> is fast, complete, and continuous (by design), it’s an <em>easy</em> optimisation problem - we can use <a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>. For this example (100-dimensional latent space representation of T2m, fit to a few thousand observations), it takes only about 100 iterations (i.e. about 100 calls of the generator), and about one minute on a standard laptop (no GPU).</p>
<div class="figure align-center" style="width: 95%">
<a class="reference internal image-reference" href="_images/Optimiser.jpg"><img alt="_images/Optimiser.jpg" src="_images/Optimiser.jpg" style="width: 95%;" /></a>
</div>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="optimiser.html">Details: DA by optimisation in latent space</a></li>
</ul>
</div>
<p>This optimisation search provides our function <span class="math notranslate nohighlight">\(f()\)</span> and it is simple to extend it to provide uncertainty estimates as well. Call the function several times with different starting guesses for the latent space vector <span class="math notranslate nohighlight">\(x\)</span> (and, if desired, perturbations to the observations to account for their uncertainty), and the resulting ensemble of real space fields provides a sample constrained by the observations.</p>
<p>To check that it works, we can make some pseudo-observations from a known field, and see how well we can recover the original field from just the observations:</p>
<div class="figure align-center" id="id4" style="width: 95%">
<a class="reference internal image-reference" href="_images/fit_1969.jpg"><img alt="_images/fit_1969.jpg" src="_images/fit_1969.jpg" style="width: 95%;" /></a>
<p class="caption"><span class="caption-text">Assimilation validation: bottom - original field (ERA5 T2m anomaly), top - assimilation results. Black dots mark observations assimilated, grey hatching marks regions where the result is very uncertain.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>This process works as expected. We can reconstruct the weather field precisely in regions where we have observations, and with uncertainty in regions where observations are unavailable.</p>
<p>It’s not just 2-metre air temperature - the same approach can be used over a wide range of applications.</p>
</div>
<div class="section" id="examples-of-use">
<h2>Examples of use<a class="headerlink" href="#examples-of-use" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="haduk-grid.html">You don't have to start from GCM output - working with haduk-grid</a></li>
<li class="toctree-l1"><a class="reference internal" href="conversion.html">Assimilating things other than observations - dataset to dataset conversion</a></li>
<li class="toctree-l1"><a class="reference internal" href="mslp.html">Not just T2m - assimilating mslp</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-variable.html">Not just one variable - finding mslp by assimilating wind observations</a></li>
</ul>
</div>
<p>How far can we go with this method? In principle the same method could be used for very large and complex sets of weather and observation fields (including satellite radiances). We can be confident that suitable generator functions are possible because they would be neural network approximations to GCMs that already exist. But it’s not yet clear how difficult it would be to train such functions.</p>
</div>
<div class="section" id="conclusions">
<h2>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">¶</a></h2>
<p>Machine Learning makes data assimilation easy, cheap, and fast.</p>
<p>Data assimilation means finding a weather field <span class="math notranslate nohighlight">\(z\)</span> that is the best fit to a set of observations <span class="math notranslate nohighlight">\(y^o\)</span>. This means maximising</p>
<div class="math notranslate nohighlight">
\[p(y^o|z)p(z)\]</div>
<p>and this is difficult because <span class="math notranslate nohighlight">\(p(z)\)</span>, the prior probability of any particular weather field, is hard to calculate. Most fields of temperature, wind etc. are physically or dynamically impossible - they don’t represent weather states that could occur in reality (i.e. <span class="math notranslate nohighlight">\(p(z)=0\)</span>). So we can’t find a maximum of this expression by searching in z - we’d need a fast functional representation of <span class="math notranslate nohighlight">\(p(z)\)</span>, and we don’t have one.</p>
<p>Machine learning provides us with a function <span class="math notranslate nohighlight">\(z=g(x)\)</span> that lets us transform the problem into the latent space <span class="math notranslate nohighlight">\(x\)</span>, where <span class="math notranslate nohighlight">\(p(x)\)</span> is an easy function. So maximising</p>
<div class="math notranslate nohighlight">
\[p(y^o|x)p(x)\]</div>
<p>is straightforward. Producing such a function <span class="math notranslate nohighlight">\(g()\)</span> by traditional methods would be extraordinarily difficult, but we can <em>learn</em> such a function quite easily, with a Variational Autoencoder trained on observations.</p>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="#">
              <img class="logo" src="_static/reconstructed.png" alt="Logo"/>
            </a></p>
<h3><a href="#">Table of Contents</a></h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model.html">Details: Building and training the VAE</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="optimiser.html">Details: DA by optimisation in latent space</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="haduk-grid.html">You don't have to start from GCM output - working with haduk-grid</a></li>
<li class="toctree-l1"><a class="reference internal" href="conversion.html">Assimilating things other than observations - dataset to dataset conversion</a></li>
<li class="toctree-l1"><a class="reference internal" href="mslp.html">Not just T2m - assimilating mslp</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-variable.html">Not just one variable - finding mslp by assimilating wind observations</a></li>
</ul>
<h3><a href="https://github.com/philip-brohan/Proxy_20CR">Get a copy</a></h3>

<ul>
<li><a href="https://github.com/philip-brohan/Proxy_20CR"
           rel="nofollow">Github repository</a></li>
</ul>

<h3>Found a bug, or have a suggestion?</h3>

Please <a href="https://github.com/philip-brohan/Proxy_20CR/issues/new">raise an issue</a>.
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="model.html" title="Building a generative model"
             >next</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">ML for DA</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Machine Learning for Data Assimilation</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    </div>
  </body>
</html>